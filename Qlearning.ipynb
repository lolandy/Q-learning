{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiCmRE7z7HeNp6CbylLzJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lolandy/Q-learning/blob/main/Qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CG9pgRaD9-m",
        "outputId": "108b90da-ed00-4990-c435-cd75df3b3751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "!pip install gymnasium\n",
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"FrozenLake-v1\"\n",
        "env = gym.make(env_name, render_mode=\"ansi\", is_slippery=False)\n",
        "print(\"Observation space: \", env.observation_space)  # 16, the number of states/ tiles\n",
        "print(\"Action space:\", env.action_space)  # 4, agent can go up, down, left, or right\n",
        "# print(type(env.action_space))  outputs: <class 'gymnasium.spaces.discrete.Discrete'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UeefxbTE2qx",
        "outputId": "23662dbc-c116-4c4a-ad03-04a087ef2687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space:  Discrete(16)\n",
            "Action space: Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:  # Agent that makes random choices\n",
        "    def __init__(self):\n",
        "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
        "\n",
        "        if self.is_discrete:\n",
        "            self.action_size = env.action_space.n  # .n gets the size of the action space\n",
        "            print(\"Action size:\", self.action_size)\n",
        "        else:\n",
        "            # continuous environment, Represented with \"Box\", a continuous n-dimension space\n",
        "            # Observation and Action space represented with: Box(low, high, shape)\n",
        "            self.action_low = env.action_space.low\n",
        "            self.action_high = env.action_space.high\n",
        "            self.action_shape = env.action_space.shape  # Ex. (2,) = 1d array with 2 elements. (2,2) = 2d array, etc\n",
        "            print(\"Action range:\", self.action_low, self.action_high)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if self.is_discrete:\n",
        "            action = random.choice(range(self.action_size))\n",
        "        else:\n",
        "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
        "        return action"
      ],
      "metadata": {
        "id": "pg7y-cv6FJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Network"
      ],
      "metadata": {
        "id": "f3nSIgnxNVr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model structure:\n",
        "1.   Input layer of states\n",
        "2.   Dense/output layer of q-values for each action\n",
        "\n",
        "Basic version:\n",
        "*   No experience relay\n",
        "*   No Double NN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XFXgf-lD98B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNAgent(Agent):  # Subclass of random agent\n",
        "  def __init__(self, env, discount_rate=0.97, learning_rate=0.001):\n",
        "      super().__init__()\n",
        "      self.state_size = env.observation_space.n\n",
        "      print(\"State size:\", self.state_size)\n",
        "\n",
        "      self.eps = 1.0\n",
        "      self.discount_rate = discount_rate  # discounts future rewards over current rewards\n",
        "      self.learning_rate = learning_rate  # rate at which the Q values are nudged\n",
        "\n",
        "      # 1. optimizer, 2. Loss funtion, 3. Neural Network (Model)\n",
        "      self.optimizer = tf.keras.optimizers.Adam()\n",
        "      self.loss_func = tf.keras.losses.MeanSquaredError()\n",
        "      # Outputs the Q-values of each action given an state\n",
        "      self.q_state = tf.keras.layers.Dense(units=self.action_size, name=\"q_table\")\n",
        "\n",
        "  def model(self, state_in): # forward pass calculations\n",
        "      # state_in = 1D, onehot tensor\n",
        "      state_in = tf.one_hot(state_in, depth=self.state_size)\n",
        "      state_in = tf.expand_dims(state_in, 0)\n",
        "      # pass the input tensor through the keras layer\n",
        "      return self.q_state(state_in)\n",
        "\n",
        "  def get_action(self, state):\n",
        "      q_state = self.model(state) # gets the action Q-values for a given state\n",
        "      action_greedy = np.argmax(q_state)  # Returns the index of max q-value in the array\n",
        "      action_random = super().get_action(state)\n",
        "      # epsilon-greedy policy\n",
        "      return action_random if random.random() < self.eps else action_greedy\n",
        "\n",
        "  def train(self, experience):  # update weights and biases of model\n",
        "      # post action states\n",
        "      state, action, next_state, reward, terminated = experience\n",
        "      q_next = self.model(next_state) # gets action q-values of next state\n",
        "      q_next = np.zeros([self.action_size]) if terminated else q_next  # Sets q_next to zeros if episode is over\n",
        "      self.action_in = tf.one_hot(action, depth=self.action_size) # formats action choice for model\n",
        "      # Bellman equation: Q(s,a) = r + discount*maxQ(s',a')\n",
        "      # Note: q_target is an estimate since q_next is estimated from the model. This causes moving target problem\n",
        "      q_target = reward + self.discount_rate * np.max(q_next)\n",
        "\n",
        "      with tf.GradientTape() as tape: # calculate gradient of loss\n",
        "        q_state = self.model(state)\n",
        "        # isolate Q-value of selected action\n",
        "        self.q_action = tf.reduce_sum(tf.multiply(q_state, self.action_in), axis=1)\n",
        "        self.loss = self.loss_func(q_target, self.q_action)\n",
        "\n",
        "      # adjust weights according to the previously calculated gradient\n",
        "      self.optimizer.minimize(self.loss, self.q_state.trainable_variables, tape)\n",
        "\n",
        "      if terminated:  # reduces random exploration as time goes on\n",
        "          self.eps = self.eps * 0.99\n",
        "\n",
        "agent = QNAgent(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_NC5IlsNUu7",
        "outputId": "b23fc1f4-baf8-4507-c1df-bae21dcbd95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action size: 4\n",
            "State size: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import clear_output\n",
        "\n",
        "total_reward = 0\n",
        "# training cycle\n",
        "for ep in range(100):\n",
        "    state = env.reset()[0]\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        action = agent.get_action(state)\n",
        "        # get effects of a chosen action\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        agent.train((state, action, next_state, reward, terminated))\n",
        "        state = next_state # move on to next state after training is complete\n",
        "        total_reward += reward\n",
        "\n",
        "        # visuals\n",
        "        print(\"s:\", state, \"a:\", action)\n",
        "        print(\"Episode: {}, Total reward: {}, Eps: {}\".format(ep, total_reward, agent.eps))\n",
        "        #print(env.render())\n",
        "        #print(agent.q_state.get_weights())\n",
        "        #time.sleep(0.05)\n",
        "        clear_output(wait=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m450f7v4jiM5",
        "outputId": "41285b6a-0134-4352-f0f7-82274e25835e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s: 15 a: 2\n",
            "Episode: 99, Total reward: 87.0, Eps: 0.04904089407128576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8/14/2023: Model converges at epoch 3, total reward = 87, Eps = 0.049"
      ],
      "metadata": {
        "id": "s2Ryh12SDk9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Table"
      ],
      "metadata": {
        "id": "yQYBQzDnM4Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent(Agent):  # Subclass of random agent\n",
        "    def __init__(self, env, discount_rate=0.97, learning_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.state_size = env.observation_space.n\n",
        "        print(\"State size:\", self.state_size)\n",
        "\n",
        "        self.eps = 1.0\n",
        "        self.discount_rate = discount_rate  # discounts future rewards over current rewards\n",
        "        self.learning_rate = learning_rate  # rate at which the Q values are nudged\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):  # Creates a Q-Table with small random values\n",
        "        self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        q_state = self.q_table[state]  # Gets the q-values of the actions corresponding to a given state\n",
        "        action_greedy = np.argmax(q_state)  # Returns the index of max q-value in the array\n",
        "        action_random = super().get_action(state)\n",
        "        return action_random if random.random() < self.eps else action_greedy\n",
        "\n",
        "    def train(self, experience):  # Updates the Q-table\n",
        "        state, action, next_state, reward, terminated = experience\n",
        "        q_next = self.q_table[next_state]  # Q-values of actions for the next state\n",
        "        q_next = np.zeros([self.action_size]) if terminated else q_next  # Sets q_next to zeros if episode is over\n",
        "        q_target = reward + self.discount_rate * np.max(q_next)  # Bellman equation: Q(s,a) = r + discount*maxQ(s',a')\n",
        "\n",
        "        # q(s,a) <- q(s,a) + learning_rate*(Q(s,a) - q(s,a))\n",
        "        q_update = q_target - self.q_table[state, action]  # Q(s,a) - q(s,a), Difference between target and actual\n",
        "        self.q_table[state, action] += self.learning_rate * q_update  # pushes q-value towards the target\n",
        "\n",
        "        if terminated:  # reduces random exploration as time goes on\n",
        "            self.eps = self.eps * 0.99\n",
        "\n",
        "agent = QAgent(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApGandS9Fks_",
        "outputId": "611a70bd-7d19-4f21-8ba8-a6c008965f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action size: 4\n",
            "State size: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import clear_output\n",
        "total_reward = 0\n",
        "for ep in range(100):  # training cycle\n",
        "    state = env.reset()[0]\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, terminated, trunc, info = env.step(action)\n",
        "        agent.train((state, action, next_state, reward, terminated))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        print(\"s:\", state, \"a:\", action)\n",
        "        print(\"Episode: {}, Total reward: {}, Eps: {}\".format(ep, total_reward, agent.eps))\n",
        "        # print(env.render())\n",
        "        # print(agent.q_table)\n",
        "        # time.sleep(0.05)\n",
        "        clear_output(wait=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SpIdkGiGVHl",
        "outputId": "7a02d234-4872-4462-bbdf-2b04b8b4b99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s: 15 a: 2\n",
            "Episode: 99, Total reward: 95.0, Eps: 0.04904089407128576\n"
          ]
        }
      ]
    }
  ]
}